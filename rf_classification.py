# -*- coding: utf-8 -*-
"""RF classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b5kI9SMOo5B2wUeN11LSqXJIqcU_SG19
"""

import os
import re
import sys
import nltk
import itertools
import numpy as np
import pandas as pd
import seaborn as sns
import joblib
import pickle
from sklearn import tree
from sklearn.svm import SVC
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Reading the Dataset
df = pd.read_csv(r'/content/dataset.csv')

# Pre Processing
def clean_resume_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove words with small lengths
    words = [word for word in text.split() if len(word) >= 3]
    text = " ".join(words)

    # Remove punctuations
    ps = list(";?.:!,")
    for p in ps:
        text = text.replace(p, '')

    # Remove extra spaces, quoting text, etc.
    text = text.replace("    ", " ").replace('"', '').replace('\t', ' ')
    text = text.replace("'s", "").replace('\n', ' ')

    # Lemmatization
    wl = WordNetLemmatizer()
    words = [wl.lemmatize(word, pos="v") for word in text.split()]
    text = " ".join(words)

    # Remove Stop-words
    sw = list(stopwords.words('english'))
    for s in sw:
        rs = r"\b" + s + r"\b"
        text = re.sub(rs, '', text)

    return text

# Apply cleaning to all resumes
df['Resume'] = df['Resume'].apply(clean_resume_text)

# Encode the Category Labels
label_encoder = LabelEncoder()
df['RoleEncoded'] = label_encoder.fit_transform(df['Role'])
y = df['RoleEncoded']  # This is the target variable we'll use

# Create a mapping dictionary for categories (for use in upload_resume.py)
role_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))
with open('role_mapping.pickle', 'wb') as f:
    pickle.dump(role_mapping, f)

# Convert Text Data into Numerical Form using TF-IDF
vectorizer = TfidfVectorizer(max_features=20000)
X = vectorizer.fit_transform(df['Resume'])

# Save TF-IDF Vectorizer as cv.pickle (this is what upload_resume.py expects)
with open('cv.pickle', 'wb') as f:
    pickle.dump(vectorizer, f)

# Split into Training and Testing Sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Save Model as RF.joblib (to match upload_resume.py expectations)
joblib.dump(rf_model, 'RFC.joblib')

# Predictions and Evaluation
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")
# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))